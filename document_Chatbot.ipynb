{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 环境介绍\n",
        "本示例运行在windows，使用cpu\n",
        "\n",
        "# 本示例概要\n",
        "本示例主要是带领大家如何讓本地端建置的LLM可以跟PDF或是docx等等文件格式進行互動，這範例的應用可以是在企業裡可以提供客戶支持或教育應用等等，那我们接着来实作吧。一下範例在之前的範例有安裝過，在這邊就不額外說明，請參考之前的範例安裝說明。\n",
        "\n",
        "\n",
        "# 范例说明\n",
        "\n",
        "1️⃣ 在Anaconda上安装相关套件\n",
        "\n",
        "\n",
        "> pip install unstructured\n",
        "\n",
        "2️⃣ 范例代码\n",
        "\n",
        "\n",
        "> 首先我们先把页面的代码进行撰写，并透过streamlit来查看页面（在脚本中不要使用\"\"\"说明\"\"\"这种备注，因为在页面中会出现你备注的文字，所以我这里备注都采取#）\n",
        "\n"
      ],
      "metadata": {
        "id": "ecfpMcoPMlNF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6ava4bNMdrR"
      },
      "outputs": [],
      "source": [
        "#強制你的腳本接著啟動 Ollama 伺服端（或該進程中載入的其他深度學習庫）時，就會以 CPU 模式運作。如果你是GPU可以拿掉此代碼\n",
        "# import os\n",
        "# import subprocess\n",
        "\n",
        "# # 設定環境變數，確保 Ollama 伺服端以 CPU 模式啟動\n",
        "# env = os.environ.copy()\n",
        "# env[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "# # 啟動 Ollama 伺服端\n",
        "# subprocess.Popen([\"ollama\", \"serve\"], env=env)\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "import time\n",
        "#导入相关语言链接库模型\n",
        "from langchain.prompts import PromptTemplate\n",
        "#PromptTemplate 則是幫你把「提示模板 (prompt template)」和「動態參數」結合起來，做到可重複、可維護、可彈性插入上下文的功能。\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# 記憶體 (Memory) 模組主要功能是在對話過程中，能夠紀錄並回傳歷史對話內容或特定資訊，讓模型在後續回合時可以參考先前上下文。\n",
        "# 常見的幾種記憶體類型：\n",
        "# ConversationBufferMemory\n",
        "\n",
        "# 將對話過程「完整」地存起來（類似累積的聊天紀錄）。\n",
        "# 在下一次呼叫 Chain（或模型）時，會把所有對話紀錄（或你指定要包含的內容）注入 Prompt 中。\n",
        "# 特性：實作簡單，但隨著對話輪數增加，Prompt 也會變得非常長。可能會有 Token 用量限制的問題。\n",
        "# ConversationBufferWindowMemory\n",
        "\n",
        "# 跟 ConversationBufferMemory 類似，但只保留「最近 N 輪」的對話內容，避免 Prompt 過度冗長。\n",
        "# 適用於對話過程較長時，可以設定一個「視窗大小 (window size)」，只把最近幾輪對話放進 Prompt。\n",
        "# ConversationSummaryMemory\n",
        "\n",
        "# 隨著對話進行，會把先前對話歸納成更精簡的摘要。\n",
        "# 下次呼叫模型時，給模型的提示就不會是全部對話文字，而是已摘要過的內容。\n",
        "# 適合長對話的場景，既能保留上下文意義，又不會導致 Token 過量。\n",
        "# ConversationKGMemory\n",
        "\n",
        "# 透過知識圖譜 (Knowledge Graph) 的形式，動態記錄對話中提到的人、事、物，以及它們之間的關係。\n",
        "# 更適合需要複雜推理、人物/事件追蹤的應用。\n",
        "\n",
        "#官方\n",
        "#from langchain.vectorstores import Chroma\n",
        "#社群版\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "\n",
        "\n",
        "# Chroma 是一款開源的向量資料庫（或嵌入式向量存儲），能夠將你的文本資料轉為向量後儲存，再使用相似度檢索的方式找到最相關的片段。\n",
        "# chroma 這個類別或函式，就代表你要使用 Chroma 來保存與檢索文本向量，用於 RAG（檢索增強生成）流程中。\n",
        "\n",
        "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
        "\n",
        "# 表示你正在使用社群版的 LangChain 擴充包，來支援 Ollama 這類模型做 Embeddings（向量化）。\n",
        "\n",
        "# Ollama 本身是一種可以在本地端推理的模型或工具，具備產生向量嵌入的能力。\n",
        "# 這種擴充包通常是社群貢獻，讓 LangChain 更加多元，無論是開源大模型還是商業 API，都能整合。\n",
        "\n",
        "from langchain_community.llms import Ollama\n",
        "#使用 LangChain 社群實作的某個 LLM 接口，與 Ollama 搭配。\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# LangChain 也提供「Callback」功能，可以在執行 Chain 或呼叫模型的每個步驟時，觸發特定事件。\n",
        "\n",
        "# 例如 StreamingStdOutCallbackHandler 可以讓 LLM 產生內容時，實時地在終端機（console）中呈現文字，方便即時觀察模型的回應過程。\n",
        "# 這對於除錯、監控或做互動式應用（例如 Chatbot）特別有用。\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "\n",
        "\n",
        "# 在 LangChain 的生態系中，CallbackManager 負責「管理整個執行流程中的回呼 (callback)」，也就是在某些關鍵事件（例如模型產生 Token、Chain 開始或結束、Agent 做出決策等）時，執行特定的回呼函式（callback function）。這些回呼可以用來做各式各樣的事情，例如：\n",
        "\n",
        "# 即時監控：在產生 Token 或 Chain 執行時，馬上將中間結果輸出到前端介面，實現串流式回應 (streaming)。\n",
        "# 記錄 (logging) 或除錯 (debug)：將執行過程中發生的事件紀錄下來，方便後續檢查與除錯。\n",
        "# 自訂行為：在每個事件點注入額外的動作，例如寫入資料庫、更新進度條、觸發通知等等。\n",
        "from langchain_community.document_loaders import (PyPDFLoader,UnstructuredWordDocumentLoader,UnstructuredPowerPointLoader,UnstructuredFileLoader)\n",
        "\n",
        "#主要用於將 文件轉換成可被大模型或語言處理管線所用的結構化文本\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# RecursiveCharacterTextSplitter 在 LangChain 中扮演文件前處理的重要角色，能智能地切分文本，讓語言模型在處理或檢索時更有效率且具可讀性。\n",
        "# 它採用「從大到小、遞迴式」的方式逐層切分，盡量保留原文的語義結構，同時尊重 chunk_size 的限制。\n",
        "# 在實際應用中，你可以根據不同文件類型（如技術文檔、聊天記錄、新聞稿、小說等）的特性，自訂分隔符與切分參數，並持續迭代測試，以找到最適合的切分策略。\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# RetrievalQA 是 LangChain 提供的一個高階 Chain，旨在把「檢索外部資料」與「透過 LLM 生成答案」這兩個核心過程一站式整合，非常適合用於需要「文件問答」或「根據外部資料產生回應」的應用場景。\n",
        "# 它完美體現了 RAG (Retrieval-Augmented Generation) 的思路：先檢索，後生成，確保回答更加準確且有根據，而不只是依賴模型的「內建知識」。\n",
        "# 只要你準備好合適的向量資料庫（或其他檢索機制）及LLM，再使用幾行程式碼就能迅速搭建一個檢索式問答服務。\n",
        "\n",
        "\n",
        "#確保文件夾存在，若不存在則創建\n",
        "pdf_directory ='pdfFiles'\n",
        "vector_directory = 'vectorDB'\n",
        "os.makedirs(pdf_directory,exist_ok=True)\n",
        "os.makedirs(vector_directory,exist_ok=True)\n",
        "\n",
        "#初始化 Streamlit 會話狀態，定義聊天機器人的提示模板,定義了機器人如何回答使用者的模板。\n",
        "if 'chat_template' not in st.session_state:\n",
        "    st.session_state.chat_template=\"\"\"您是一個協助解答文檔中問題的聊天機器人，根據context回答問題。若無法回答則表示'根據資料無法回答此問題'。\n",
        "\n",
        "    Context: {context}\n",
        "    History: {history}\n",
        "    User: {question}\n",
        "    Chatbot:\"\"\"\n",
        "\n",
        "#建立 PromptTemplate，指定prompt接受的格式,將上面的提示模板與歷史對話紀錄、內容及使用者問題進行結合。\n",
        "if 'prompt_template' not in st.session_state:\n",
        "    st.session_state.prompt_template = PromptTemplate(\n",
        "        input_variables=[\"history\",\"context\",\"question\"],\n",
        "        template = st.session_state.chat_template,\n",
        "        )\n",
        "\n",
        "#指定上下文存儲的範,用於儲存歷史聊天記錄。ConversationBufferMemory可以參考上面備註\n",
        "if 'conversation_memory' not in st.session_state:\n",
        "    st.session_state.conversation_memory=ConversationBufferMemory(\n",
        "        memory_key=\"history\",\n",
        "        return_messages=True,\n",
        "        input_key=\"question\",\n",
        "        )\n",
        "\n",
        "#建立向量資料庫 Chroma，指定專程向量的編碼。建立 Chroma 資料庫並設定 embedding 模型為 Ollama。\n",
        "if 'embeddings_store' not in st.session_state:\n",
        "    st.session_state.embeddings_store=Chroma(\n",
        "        persist_directory=vector_directory,\n",
        "        embedding_function=OllamaEmbeddings(\n",
        "            base_url='http://localhost:11434',\n",
        "            model=\"mxbai-embed-large\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "#指定語言模型\n",
        "\n",
        "# 這個範例使用的模型可以考慮切換對中文適洽較高的模型\n",
        "\n",
        "# 「回呼（callback）」可以想像成事件觸發後要去執行的動作。例如模型在一步一步生成文字的過程中，程式可以在每產生一段字串時「呼叫」一些自訂的函式來處理或顯示這些資訊。\n",
        "\n",
        "# CallbackManager：一個可以放多個「回呼處理器（callback handlers）」的管理器。當模型產生新的文字或完成某些階段時，這個管理器就會呼叫其中所有的回呼處理器。\n",
        "# StreamingStdOutCallbackHandler()：一個專門用來在推論過程中「即時」把文字輸出到畫面（或終端機）的回呼處理器。它收到新產生的文字時，就會立刻把這些文字印到標準輸出（終端機）。\n",
        "#建立 Ollama 語言模型,使用 Ollama 的 llama3 模型作為主要大模型，提供流式回覆功能。\n",
        "if 'language_model' not in st.session_state:\n",
        "    st.session_state.language_model = Ollama(base_url=\"http://localhost:11434\",\n",
        "                                             model=\"llama3\",\n",
        "                                             verbose=True,\n",
        "                                             callback_manager=CallbackManager(\n",
        "                                                 [StreamingStdOutCallbackHandler()]),\n",
        "                                             )\n",
        "#用來「記住」某些資料或狀態，避免每次重新執行都把之前的內容遺失掉。\n",
        "if 'chat_history' not in st.session_state:\n",
        "    st.session_state.chat_history=[]\n",
        "\n",
        "\n",
        "#主標題\n",
        "st.title(\"Chatbot With More Files\")\n",
        "\n",
        "#PDF 文件上傳器\n",
        "uploaded_file = st.file_uploader(\"請選擇檔案\", type=[\"pdf\", \"docx\", \"pptx\", \"txt\"])\n",
        "\n",
        "#顯示聊天歷史\n",
        "for message in st.session_state.chat_history:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"message\"])\n",
        "\n",
        "\n",
        "#處理上傳的PDF文件，對於chunk_size和chunk_overlap可以根據你的硬體需求以及你對解析的精準度作為調整\n",
        "# 處理上傳的 PDF 文件\n",
        "if uploaded_file is not None:\n",
        "    file_path = os.path.join(pdf_directory, uploaded_file.name)\n",
        "    if not os.path.exists(file_path):\n",
        "        with st.status(\"Saving file...\"):\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(uploaded_file.getvalue())\n",
        "\n",
        "            file_extension = uploaded_file.name.split(\".\")[-1]\n",
        "            if file_extension.lower() == \"pdf\":\n",
        "                # 處理PDF檔案的程式碼\n",
        "                loader = PyPDFLoader(file_path)\n",
        "            elif file_extension.lower() == \"docx\":\n",
        "                # 處理DOCX檔案的程式碼\n",
        "                loader = UnstructuredWordDocumentLoader(file_path)\n",
        "            elif file_extension.lower() == \"pptx\":\n",
        "                # 處理PPTX檔案的程式碼\n",
        "                loader = UnstructuredPowerPointLoader(file_path)\n",
        "            elif file_extension.lower() == \"txt\":\n",
        "                # 處理TXT檔案的程式碼\n",
        "                loader = UnstructuredFileLoader(file_path)\n",
        "\n",
        "\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1500,\n",
        "                chunk_overlap=200,\n",
        "                length_function=len\n",
        "            )\n",
        "\n",
        "            doc = loader.load_and_split(text_splitter)\n",
        "\n",
        "            st.session_state.embeddings_store = Chroma.from_documents(\n",
        "                documents=doc,\n",
        "                embedding=OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
        "            )\n",
        "            st.session_state.embeddings_store.persist()\n",
        "\n",
        "\n",
        "    #啟動PDF文檔檢索\n",
        "    st.session_state.retriever = st.session_state.embeddings_store.as_retriever()\n",
        "\n",
        "    #啟動 PDF 檢索器與問答鏈 (QA Chain),建立能根據使用者提問從 PDF 中找到答案的問答鏈。\n",
        "    if 'qa_chain' not in st.session_state:\n",
        "        st.session_state.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=st.session_state.language_model,\n",
        "            chain_type='stuff',\n",
        "            retriever=st.session_state.retriever,\n",
        "            verbose=True,\n",
        "            chain_type_kwargs={\n",
        "                \"verbose\": True,\n",
        "                \"prompt\":st.session_state.prompt_template,\n",
        "                \"memory\":st.session_state.conversation_memory,\n",
        "                }\n",
        "            )\n",
        "    #處理使用者輸入並即時生成回覆,接受使用者提問，呼叫問答鏈獲得回答，並逐字模擬輸入效果即時顯示回答。\n",
        "    if user_input := st.chat_input(\"You:\",key=\"user_input\"):\n",
        "        user_message={\"role\":\"user\",\"message\":user_input}\n",
        "        st.session_state.chat_history.append(user_message)\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(user_input)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"助理正在輸入...\"):\n",
        "                response = st.session_state.qa_chain(user_input)\n",
        "            message_placeholder = st.empty()\n",
        "            full_response=\"\"\n",
        "            for chunk in response['result'].split():\n",
        "                full_response += chunk + \"\"\n",
        "                time.sleep(0.05)\n",
        "                #Add a blinking cursor to simulate typing\n",
        "                message_placeholder.markdown(full_response + \"▌\")\n",
        "            message_placeholder.markdown(full_response)\n",
        "        chatbot_message = {\"role\":\"assistant\",\"message\":response['result']}\n",
        "        st.session_state.chat_history.append(chatbot_message)\n",
        "\n",
        "else:\n",
        "    st.write(\"請上傳一個PDF檔案來開始 ChatPDF\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "再来继续在Anaconda Prompt执行python -m streamlit run 您的程序名称。这里要注意streamlit版本，执行语法可能会有差异。\n",
        "\n",
        "2.   打开Anaconda Prompt切换到上面python的脚本位置，执行cd python的脚本位置指令。\n",
        "\n",
        "\n",
        "> cd C:\\Users\\joyce\n",
        "\n",
        "\n",
        "3.   你的第一个本地端自然语言对话程式就完成。\n",
        "\n",
        "\n",
        "> python -m streamlit run documChatbot.py"
      ],
      "metadata": {
        "id": "-u771yJ_M9TO"
      }
    }
  ]
}